# 并行计算 II 2023 春季第一次作业：<br> OpenMP 优化 Bellman-Ford 算法

## 1. Bellman-Ford 算法
    单源最短路径，以0点为起点，通过邻接矩阵更新距离向量  
    输入：邻接矩阵mat  
    输出：距离向量d  
    算法：  
        初始化：d[0]=0  
        状态转移：d[x] = min(d[x], d[y]+w[y][x])  
        终止条件：在某次迭代后没有发生更新，或已迭代了n-1轮。  
            此时需要多迭代一轮，判定负权。

## 2. 串行计算
    见serial.cpp

## 3. 并行计算
* 基本分析
    > 状态转移：d[x] = min(d[x], d[y]+w[y][x])  
    > 基于上述状态转移过程，各状态可以独立计算。但必须注意：  
    > 1. 最外层循环即迭代轮数不可作并行，因为这是顺序依赖的，而两层内层循环则可以**选择性地作并行**。如何设置并行区？可以在每轮迭代时开辟并行区，也可以使用持久化的并行区。
    > 2. 算法的一个终止条件是基于最短路径长度不超过n，这要求我们必须保证遍历过路径长为n的所有路径，对变量之间限定了一定的依赖关系。如果是完全独立地计算，可能某一点在计算了n步后其他点都还未开始，这将导致不能给出正确的结果。因此，在每轮迭代后需要作**栅栏同步**，此外，对has_change的共享也需要同步。
    > 3. 对串行计算而言，每轮迭代更新的内层两重循环——工作划分并无显著区别。而对并行计算而言，**工作划分是有意义的**。


## 4. 因素比较
* 流程
    ```zsh
    # 记得在编译时添加选项 -fopenmp
    make
    # 记得在脚本中指定偏好节点
    sbatch run.slurm
    ```
* 串行计算

    见serial.cpp。平均时间约0.24s
* 核心数和线程数
    1. 单核  
        > 平均时间约0.3s，比串行计算还慢。因为在一个核心上多线程计算实际上还是串行执行的，还需要额外调度的时间。
    2. 多核  
        ```zsh
        # 登录节点
        sinfo --Node --long
        scontrol show node
        # 计算节点
        lscpu
        ```
        > 在登录节点使用sinfo命令和scontrol命令可以查看各节点粗略信息。  
        > 在计算节点使用slurm脚本获取该节点详细信息  
        > 
        > 线程数最好与硬件核心数匹配，这样能最大限度地并行化，考虑包括：  
        > 1. 是否支持超线程？若是，则一个核心可以运行多个线程以增加并行度（但达不到多核的并行度）。数院集群使用的CPU是E5-2650，双路12核单线程。
        > 2. 一个节点的核心是单路还是多路？如果是多路，需要权衡访存一致性和硬件资源的优劣：各路之间不共享缓存，但共享内存，对同一片数据并行操作会增加延迟。而增加路数可以使计算资源加倍。本问题是访存密集型的，故只是用一路12核CPU。
        > 3. 虽然每个节点的CPU数基本都是24核，但在提交批处理脚本时不允许申请这么多个核心。实验中，申请12核的性能不太稳定，这是由于默认申请的节点cu01还有其他事务共享计算资源而发生调度，所以在性能下降。因此，后续在作业脚本命令中用"-w cuXX"指定节点cuXX 。

    经过实验，在核心数和线程数均为12时，速度最快。  
    可以加速到约**5倍**。

* 临时或持久的并行区
    > 最简单的做法是每轮迭代都开辟一个并行区，并行区内划分循环，但每轮都需要初始化并行区，这可能引入额外的初始化开销。  
    > 可以尝试使用持久化的并行区，对内层循环作任务分配，在不同轮次间作一次栅栏同步即可。

    经过实验，持久化的并行区效率更高。

* 并行划分
    1. 工作分配

        在一轮迭代中：对于两层循环，在并行划分时可以选择针对内层或者外层作分配。
        ```cpp
        for (int v = tid; v < n; v += nt)   // 分配
        {
            // 从内层提取的复用片段
            for (int u = 0; u < n; u ++)    // 完全遍历
            {
                dist[v] = dist[u] + utils::mat[u][v];
            }
        }
        ```  
        > 如果两层循环是直接嵌套的，那么分配方式区别不大。  
        > 不过，如果内层循环（工作集）中存在复用的代码段，则可以提取到外层一次性完成，以减少重复执行次数。  
        > 只要缓存能存得下， 就尽可能地使用更大的工作集，以充分节约重复操作。  

        因此，应该在外层作分配，而使内层完全循环。

    2. 遍历方式
        > 状态转移：d[x] = min(d[x], d[y]+w[y][x])
        1. 终点优先
            > 在一轮迭代中，每个线程保持终点不变，只对遍历的途径点作并行，即并行遍历y。  
            > 这样的好处是每个线程每次独立维护自己的可变变量d[x]。
        2. 途径点优先
            > 在一轮迭代中，每个线程保持途径点不变，只对遍历的终点作并行，即并行遍历x。  
            > 这样的好处是每个线程每次重复访问同一个只读变量d[y]。
        3. 循环展开 
            > 在一轮迭代中，同时对终点和途径点作并行，最简单的方法就是使用collapse从句。
            > 好处是更高的视角实现更平均的循环分配（负载更均衡），但问题是这样就不支持前述的数据复用了。

        经过实验，途径点优先的策略速度更快（见第3条——缓存）。
    2. 调度策略
        > 如果一层循环遍历从1到n需要作并行任务划分，有两种常见静态调度策略：round-robin和range。  
        > 理论上说，使用range策略可以让每个线程连续访问一块内存，能有效利用空间局部性（缓存线）。  
        > 但是实际上的测试结果：缓存u策略下round-robin效果更好；缓存v策略下，二者无明显区别。  
        > 我并没有思考出一个合理的解释。
        结论：round-robin的效果更好。
    
    充分尝试各种划分-遍历策略，发现最优的情况可以把并行算法加速至约**6至7倍**。

* 缓存
    > 鉴于一个线程经常访问同一个变量，因此可以对重复访问的部分作缓存优化。虽然主动缓存也需要额外开销，但在访问次数较多时还是能够加速。  
    > 可能复用的变量包括：has_change, dist[v], dist[u], mat[u][v]。  
    > has_change缓存：每个线程可以维护一个私有变量local_has_change，最后再作规约，以减小维护共享变量一致性的代价。  
    > mat[u][v]批量缓存：可以转置矩阵，故不论是按u还是按v遍历都可以做缓存。  
    > dist[u]缓存：这要求任务按u进行划分，每个线程重复访问各自的u，以利用数据的时间局部性。缓存减少了从更外层存储器读的次数。  
    > dist[v]缓存：这要求任务按v进行划分，每个线程独立维护各自的v，以保证数据安全性。缓存减少了对更外层存储器(L3cache)写的次数，但由于是条件写，次数可能不多。  
    > dist批量缓存：对dist[v]不能一次性缓存，因为每个dist[v]未必发生修改，批量写回会产生不必要的访存；对dist[u]的批量缓存会解除原有“非规则的”数据依赖（在一轮迭代中，可能后续计算会使用到本轮更新过的值，而不是原有值）。这样迭代快但更新量小，很难判断哪一种策略效率更高。  
    > 此外，由于first touch原则，在分配共享内存后，还应该在各线程上分别赋初值，以抵消内存的延迟分配。  
    实验发现，对has_change和mat[u][v]作缓存都可以有效加速。缓存dist[u]比缓存dist[v]速度更快，不过在充分优化后，二者的效果基本被拉平。  

    使用局部空间和临时变量可以额外加速大约**2倍**。

* 存储方式
    > 默认的存储方式是邻接矩阵mat[u][v]。此外，还可以存储转置矩阵和邻接表。  
    > 转置矩阵matT[v][u]的应用场景是按批量按列访问时可以一次缓存一组。  
    > 邻接表table[u][index]的应用场景是当图较稀疏时，可以减少对空边的无效访问，但要额外保存v作为存储代价。在本问题中，尽管额外的空间代价不至于造成缓存满溢，但图是稠密的，预计使用邻接表的效果并不显著，不作为主要优化思路。

* 编译选项
    > 在C++代码中，无法显式控制缓存和寄存器的使用方式。前述缓存相关的讨论我只能以局部空间和临时变量的方式实现（显式指定保存到一个新的位置），以**期望**其进入缓存。而编译器层面则可以优化产生的汇编代码中寄存器的使用方式，给出更紧凑的流水线，以及其他汇编层面的优化。
    实验发现，-O2选项即有显著优化，而-O3和-Ofast没有进一步优化。  
    使用编译优化可以加速大约**2倍**（果然编译优化还是强大）。

* 正确性检查
    > 将输出output.txt与串行结果output_std.txt进行比对  
    > 使用./check.cpp计算正确性，各策略结果均正确

* 时长比较
    ```text
    serial
    Time(s): 0.242699
    PASSED
    destination
    Time(s): 0.051606
    PASSED
    pathway
    Time(s): 0.041085
    PASSED
    collapse
    Time(s): 0.040408
    PASSED
    pathway_cache
    Time(s): 0.037081
    PASSED
    persistent without Optimization
    Time(s): 0.018184
    PASSED
    persistent_T
    Time(s): 0.009544
    PASSED
    persistent-final
    Time(s): 0.008936
    PASSED
    ```
    > 以上是执行一次脚本所反馈的结果：  
    > serial是串行方法，destination是固定终点遍历途径点的策略，pathway是固定途径点遍历终点的策略，collapse是将两层循环统筹分配的策略。  
    > pathway_cache引入了一部分缓存尝试，persisten_T和persisten都使用了持久化的并行区并进一步尝试了缓存，二者区别和pathway & destination的区别类似（互为转置）。  
    > persistent without Optimization及前面的尝试未使用编译优化，后续两个尝试都使用了-Ofast选项
## 5. 总结
* 最终方案  
    直观上看，上述几种因素大致是**正交**的，所以最自然的方案是在每种因素上选择最优方案，组合起来即可。因此，最终的方案persistent.cpp使用了如下选项：
    1. 线程数为12。在一轮迭代中，每个线程负责整个u-v中的1/12的遍历，即每轮任务负载N^2/12。  
    2. 外层循环作划分，内层循环完全遍历
    3. 外层循环遍历u（途径点），内层循环遍历v（终点）
    4. 分配策略为round-robin
    5. 使用局部空间和临时变量“引导”缓存
    6. 对共享内存并行赋初值以抵消内存延迟分配
    7. 编译语句中使用-Ofast选项进行优化  
        ```bash
        g++ -std=c++11 -o final persistent.cpp  -Ofast -fopenmp
        ```
* 测试方法  
    ```bash
    make
    sbatch run.slurm
    ```
* 优化效果  
    在前一节的测试结果中，取serial和persistent-final的计时结果进行比较。
    ```
    serial
    Time(s): 0.242657
    persistent-final
    Time(s): 0.008858
    ```
    可以算出最终加速了约**27倍**。  
    因素分解：在前一部分各因素测试中，并行任务划分优化了约7倍，持久化并行区和缓存技术优化了约2倍，编译器优化了约2倍。